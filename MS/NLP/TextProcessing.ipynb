{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/irikos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import wikipedia\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from pycontractions import Contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from num2words import num2words\n",
    "\n",
    "# need to be local and downloaded. Current path assumes it is in the same folder\n",
    "# download from: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "cont = Contractions('GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model not found at GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irikos/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# takes a bit\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're gonna use the Elon Musk page in this homework\n",
    "# note: the page cannot be found by 'Elon Musk', as is the link\n",
    "page = wikipedia.page('Musk Elon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk\n",
      "['Elon', 'Reeve', 'Musk', 'born', 'June', '28', '1971', 'is', 'an', 'engineer', 'industrial', 'designer', 'philanthropist', 'and', 'technology', 'entrepreneur', 'He', 'is', 'a', 'citizen', 'of', 'South', 'Africa', 'the', 'United', 'States', 'where', 'he', 'has', 'lived', 'most', 'of', 'his', 'life', 'and', 'currently', 'resides', 'and', 'Canada', 'He', 'is', 'the', 'founder', 'CEO', 'and', 'chief', 'engineer', 'designer', 'of', 'SpaceX', 'early', 'investor', 'CEO', 'and', 'product', 'architect', 'of', 'Tesla', 'Inc', 'founder', 'of', 'The', 'Boring', 'Company', 'co', 'founder', 'of', 'Neuralink', 'and', 'co', 'founder', 'and', 'initial', 'co', 'chairman', 'of', 'OpenAI', 'He', 'was', 'elected', 'a', 'Fellow', 'of', 'the', 'Royal', 'Society', 'FRS', 'in', '2018', 'In', 'December', '2016', 'he', 'was', 'ranked', '21st', 'on', 'the', 'Forbes', 'list', 'of', 'The', 'World', 's', 'Most', 'Powerful', 'People', 'and', 'was', 'ranked', 'joint', 'first', 'on', 'the', 'Forbes', 'list', 'of', 'the', 'Most', 'Innovative', 'Leaders', 'of', '2019', 'He', 'has', 'a', 'net', 'worth', 'of', '27', '9', 'billion', 'and', 'is', 'listed', 'by', 'Forbes', 'as', 'the', '20th', 'richest', 'person', 'in', 'the', 'world', 'He', 'is', 'the', 'longest', 'tenured', 'CEO', 'of', 'any', 'automotive', 'manufacturer', 'globally', 'Born', 'and', 'raised', 'in', 'Pretoria', 'South', 'Africa', 'Musk', 'briefly', 'attended', 'the', 'University', 'of', 'Pretoria', 'before', 'moving', 'to', 'Canada', 'when', 'he', 'was', '17', 'to', 'attend', 'Queen', 's', 'University', 'He', 'transferred', 'to', 'the', 'University', 'of', 'Pennsylvania', 'two', 'years', 'later', 'where', 'he', 'received', 'a', 'bachelor', 's', 'degree']\n"
     ]
    }
   ],
   "source": [
    "#1. Choose a wikipedia article. You will download and acces the article using this python module: wikipedia. \n",
    "# Use the content property to extract the text. Print the title of the article \n",
    "content = page.content\n",
    "print(page.title)\n",
    "\n",
    "# and the first N=200 words from the article (use the tokenizer).\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "only_words = tokenizer.tokenize(content)\n",
    "print(only_words[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Expand language contractions (for example change \"we're\" in \"we are\") using pycontractions\n",
    "# takes A LOT, with or without precise=True\n",
    "content = list(cont.expand_texts([content], precise=True))\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a list of all the words (in lower case), from the text, using the word tokenizer. \n",
    "# We will name the list l_words. Remove the punctuation.\n",
    "# weird to ask this since it ruins a bit exercise 6.\n",
    "words = nltk.word_tokenize(content)\n",
    "l_words = [word.lower() for word in words if word.isalnum()]\n",
    "print(l_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Remove stopwords from l_words and print the number of words in the text, before and after removing stopwords.\n",
    "print(len(l_words)) # list with stopwords\n",
    "sw = stopwords.words('english')\n",
    "l_words_sw = [word for word in l_words if word not in stopwords.words('english')]\n",
    "print(len(l_words_sw)) # list without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Using RegexpTokenizer (nltk) please obtain the list of words written with capital letter that don't \n",
    "# appear in the beginning of the phrase (name this list lnm).\n",
    "# not exactly clear if this applies per-sentence (aka add a word that does not appear in THIS sentence in the beginning)\n",
    "# also not clear if we should add duplicates as well, as we progress\n",
    "cap_tokenizer = nltk.RegexpTokenizer('(?<!^)(?<!^ )(?<!\\!)(?<!\\! )(?<!\\?)(?<!\\? )(?<!\\.)(?<!\\. )[A-Z][a-z]+')\n",
    "\n",
    "capitalized = list(cap_tokenizer.tokenize(content))\n",
    "\n",
    "print(capitalized)\n",
    "\n",
    "# exercise before regex\n",
    "#sentences = nltk.sent_tokenize(content)\n",
    "#lnm = []\n",
    "#for sentence in sentences:\n",
    "#    sentence_words = nltk.word_tokenize(sentence)\n",
    "#    for word in sentence_words:\n",
    "#        if (word != sentence_words[0] and word.isalnum() and word == word.capitalize()):\n",
    "#            lnm.append(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Remove all entity names (words that appear with capital letter inside the phrase, not at the beginning from l_words).\n",
    "no_entity_l_words = [word for word in l_words if word.capitalize() not in lnm] # used capitalize() because in ex #2 we have them all lowercase in l_words\n",
    "print(len(no_entity_l_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Apply a stemmer (Snowball) on each of the word from the list of words, l_words (without changing the list) and print the result.\n",
    "snb=nltk.SnowballStemmer(\"english\")\n",
    "l_words_snb = [snb.stem(word) for word in l_words]\n",
    "\n",
    "print (l_words_snb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.Apply a lematizer on each of the word from the list of words, l_words (without changing the list) and print the result. \n",
    "# Explain the different result (from the stemming process) in a comment.\n",
    "lem=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "l_words_lem = [lem.lemmatize(word) for word in l_words]\n",
    "print (l_words_lem)\n",
    "\n",
    "# stemming cuts the end of the word, taking into account common endings. This results in errors, such as 'scienc' instead of 'science.'\n",
    "# lemmitization takes into consideration the morphological analysis of the word, thus sometimes making no modification to the word\n",
    "# setmming also cuts names, such as youtube, while lemmitization leaves them in their current form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Change all the numbers from l_words into words, using num2words. \n",
    "# Print the number of changes, and also the portion of list that contains first N changes (for example N=10).\n",
    "num_words = [num2words(word) for word in l_words if (word.isnumeric())]\n",
    "\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attend\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b2d8b2b17640>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minflexion_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attend'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-b2d8b2b17640>\u001b[0m in \u001b[0;36minflexion_list\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mw_lem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mwords_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
     ]
    }
   ],
   "source": [
    "# 10. Create a function that receives a string W as parameter. \n",
    "# The function must return a list with all the sentences containing any inflexion of the word W. \n",
    "# For example for W=running, these sentences could be in the list: \"I like to run\", \"He runs the fastest\". \n",
    "# You need to apply stemming both on the word W but also on each word of the sentence.\n",
    "lem=WordNetLemmatizer()\n",
    "\n",
    "def inflexion_list(W):\n",
    "    returned_sentences = []\n",
    "    add_to_list = False\n",
    "    w_lem = lem.lemmatize(W)\n",
    "    print(lem.lemmatize(W))\n",
    "    sentences = nltk.sent_tokenize(content)\n",
    "    for sentence in sentences[0:10]:\n",
    "        words_list = nltk.word_tokenize(sentence)\n",
    "        for word in words_list:\n",
    "            if (w_lem == lem.lemmatize(word)):\n",
    "                add_to_list = True\n",
    "            if (add_to_list == True):\n",
    "                returned_sentences.append(sentence)\n",
    "            add_to_list = False\n",
    "    return returned_sentences\n",
    "\n",
    "\n",
    "print(inflexion_list('attend'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
