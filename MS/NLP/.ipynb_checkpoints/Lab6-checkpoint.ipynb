{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/irikos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/irikos/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "from nltk.wsd import lesk\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SODA redirect to Soda(3318069)\n",
      "Sodas redirect to Soda(5599353)\n",
      "Soday redirect to Soda(13752636)\n",
      "Soda (disambiguation) redirect to Soda(19677220)\n",
      "Soda drink redirect to Soda(47486780)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it has some errors, doesn't work\n",
    "\n",
    "def wikipedia_disambiguation(page_title):\n",
    "    disambiguation = [page_title]\n",
    "    #create a connection(session)\n",
    "    r_session = requests.Session()\n",
    "\n",
    "    #url for the MediaWiki action API\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\", #we are creating a query\n",
    "        \"titles\": \"soda\", #for the title car    \n",
    "        \"prop\": \"redirects\", #asking for all the redirects (to the title car)\n",
    "        \"format\": \"json\" #and we want the output in a json format\n",
    "    }\n",
    "\n",
    "    #we obtain the response to the get request with the given parmeters\n",
    "    query_response = r_session.get(url=URL, params=PARAMS)\n",
    "    json_data = query_response.json()\n",
    "    print()\n",
    "    wikipedia_pages = json_data[\"query\"][\"pages\"]\n",
    "    #we iterate through items and print all the redirects (their title and id)\n",
    "    try:\n",
    "        for k, v in wikipedia_pages.items():\n",
    "            for redir in v[\"redirects\"]:\n",
    "                print(\"{} redirect to {}({})\".format(redir[\"title\"], v[\"title\"], redir[\"pageid\"]))\n",
    "                disambiguation.append(v[\"title\"])\n",
    "                pageid = redir[\"pageid\"]\n",
    "                PARAMS2 = {\n",
    "                    \"action\": \"query\", #we are creating a query    \n",
    "                    \"prop\": \"info\", #asking for all the redirects (to the title car)\n",
    "                    \"format\": \"json\", #and we want the output in a json format\n",
    "                    \"pageids\": pageid\n",
    "                }\n",
    "                query_response2 = r_session.get(url=URL, params=PARAMS2)\n",
    "                json_data2 = query_response2.json()\n",
    "                wiki = json_data2[\"query\"][\"pages\"]\n",
    "                print(wiki)\n",
    "#                 print(query_response.json[\"query\"][\"pages\"][pageid][\"title\"])\n",
    "            \n",
    "    except KeyError as err:\n",
    "        if err.args[0]=='redirects':\n",
    "            print(\"It has no redirects\")\n",
    "        else:\n",
    "            print(repr(err))\n",
    "            \n",
    "    # I know it's a stupid way to do it, but couldn't find fast how to include many prop\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\", #we are creating a query\n",
    "        \"titles\": \"soda\", #for the title car    \n",
    "        \"prop\": \"categories\", #asking for all the redirects (to the title car)\n",
    "        \"format\": \"json\" #and we want the output in a json format\n",
    "    }\n",
    "\n",
    "    #we obtain the response to the get request with the given parmeters\n",
    "    query_response = r_session.get(url=URL, params=PARAMS)\n",
    "    json_data = query_response.json()\n",
    "    print()\n",
    "    wikipedia_pages = json_data[\"query\"]\n",
    "    #we iterate through items and print all the redirects (their title and id)\n",
    "    try:\n",
    "        for k, v in wikipedia_pages.items():\n",
    "            for cat in v['categories']:\n",
    "                print(cat[\"title\"]) ##### HOW DO YOU GET THE SYNTACTIC HEAD???\n",
    "                disambiguation.append(cat['title'])\n",
    "                   \n",
    "            \n",
    "    except KeyError as err:\n",
    "        if err.args[0]=='redirects':\n",
    "            print(\"It has no redirects\")\n",
    "        else:\n",
    "            print(repr(err))\n",
    "            \n",
    "    \n",
    "    \n",
    "    print(disambiguation)\n",
    "    return disambiguation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SODA redirect to Soda(3318069)\n",
      "{'3318069': {'pageid': 3318069, 'ns': 0, 'title': 'SODA', 'contentmodel': 'wikitext', 'pagelanguage': 'en', 'pagelanguagehtmlcode': 'en', 'pagelanguagedir': 'ltr', 'touched': '2020-04-23T22:29:54Z', 'lastrevid': 946494559, 'length': 63, 'redirect': ''}}\n",
      "Sodas redirect to Soda(5599353)\n",
      "{'5599353': {'pageid': 5599353, 'ns': 0, 'title': 'Sodas', 'contentmodel': 'wikitext', 'pagelanguage': 'en', 'pagelanguagehtmlcode': 'en', 'pagelanguagedir': 'ltr', 'touched': '2020-04-23T22:29:54Z', 'lastrevid': 422950905, 'length': 18, 'redirect': ''}}\n",
      "Soday redirect to Soda(13752636)\n",
      "{'13752636': {'pageid': 13752636, 'ns': 0, 'title': 'Soday', 'contentmodel': 'wikitext', 'pagelanguage': 'en', 'pagelanguagehtmlcode': 'en', 'pagelanguagedir': 'ltr', 'touched': '2020-04-23T22:29:54Z', 'lastrevid': 944798394, 'length': 18, 'redirect': ''}}\n",
      "Soda (disambiguation) redirect to Soda(19677220)\n",
      "{'19677220': {'pageid': 19677220, 'ns': 0, 'title': 'Soda (disambiguation)', 'contentmodel': 'wikitext', 'pagelanguage': 'en', 'pagelanguagehtmlcode': 'en', 'pagelanguagedir': 'ltr', 'touched': '2020-04-23T22:29:54Z', 'lastrevid': 245754104, 'length': 46, 'redirect': ''}}\n",
      "Soda drink redirect to Soda(47486780)\n",
      "{'47486780': {'pageid': 47486780, 'ns': 0, 'title': 'Soda drink', 'contentmodel': 'wikitext', 'pagelanguage': 'en', 'pagelanguagehtmlcode': 'en', 'pagelanguagedir': 'ltr', 'touched': '2020-05-29T08:51:00Z', 'lastrevid': 880350214, 'length': 104, 'redirect': ''}}\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-373-6fd2757a3293>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwikipedia_disambiguation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"soda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-372-5955a3418620>\u001b[0m in \u001b[0;36mwikipedia_disambiguation\u001b[0;34m(page_title)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwikipedia_pages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'categories'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##### HOW DO YOU GET THE SYNTACTIC HEAD???\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mdisambiguation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "wikipedia_disambiguation(\"soda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(sentence):\n",
    "    sentence_split = nltk.word_tokenize(sentence.lower())    \n",
    "    only_words = [word.lower() for word in sentence_split if word.isalnum()]\n",
    "    relevant_words = [word for word in only_words if word not in stopwords.words('english')]\n",
    "    lemmatized = [wnl.lemmatize(word) for word in relevant_words]\n",
    "    return lemmatized\n",
    "\n",
    "def wordnet_disambiguation(word, part_of_speech, number): # part_of_speech = \"n\" / \"v\" etc. No is 1 2 etc etc\n",
    "    disambiguation = []\n",
    "    synset = \"\"\n",
    "    # get the synset\n",
    "    for ss in wordnet.synsets(word):\n",
    "        if (ss.pos().lower() == part_of_speech):\n",
    "            synset = ss\n",
    "            \n",
    "    # add its synonyms\n",
    "    disambiguation += synset.lemma_names()\n",
    "    \n",
    "    # add the hypernyms and hyponyms\n",
    "    for hyp in synset.hypernyms():\n",
    "        disambiguation.append(hyp.name().split('.')[0])\n",
    "        print(hyp.root_hypernyms())\n",
    "#         print(dir(hyp))\n",
    "        \n",
    "    for hyp in synset.hyponyms():\n",
    "        disambiguation.append(hyp.name().split('.')[0])        \n",
    "    # add the gloss, keeping only the lemmatized content words\n",
    "    disambiguation += get_context(synset.definition())\n",
    "    \n",
    "    # systerhood?\n",
    "    disambiguation += synset.similar_tos()\n",
    "    print (disambiguation)\n",
    "    \n",
    "def return_all(synset):\n",
    "    syn_all = \"\"\n",
    "    for hyp in synset.hypernyms():\n",
    "        syn_all += \" \" + hyp.definition().lower()\n",
    "    for hyp in synset.hyponyms():\n",
    "        syn_all += \" \" + hyp.definition().lower()\n",
    "    for hyp in synset.part_meronyms():\n",
    "        syn_all += \" \" + hyp.definition().lower()\n",
    "    for hyp in synset.substance_meronyms():\n",
    "        syn_all += \" \" + hyp.definition().lower()\n",
    "    for hyp in synset.member_meronyms():\n",
    "        syn_all += \" \" + hyp.definition().lower()\n",
    "    for hyp in synset.similar_tos():\n",
    "        syn_all += \" \" + hyp.definition().lower()\n",
    "    for hyp in synset.also_sees():\n",
    "        syn_all += \" \" + hyp.definition().lower()\n",
    "    return syn_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('entity.n.01')]\n",
      "['pop', 'soda', 'soda_pop', 'soda_water', 'tonic', 'soft_drink', 'sweet', 'drink', 'containing', 'carbonated', 'water', 'flavoring']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('beverage.n.01')]\n"
     ]
    }
   ],
   "source": [
    "wikipedia_senses = wikipedia_disambiguation(\"soda\")\n",
    "wordnet_sense = wordnet_disambiguation(\"soda\", \"n\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_algorithm(wikipedia_sense, wordnet_senses):\n",
    "    mapping = []\n",
    "    for w in wikipedia_senses:\n",
    "        ### Who is epsilon here??\n",
    "        print(\"not sure who is epsilon to add it to the mapping\")\n",
    "    for w in wikipedia_senses:\n",
    "        if \n",
    "        \n",
    "        \n",
    "##### Algorithm from the paper. I tried understanding it, but couldn't manage. \n",
    "# I understood a bit of it, but too much doesn't really make sense (ha, pun intended)\n",
    "Input: SensesWiki, SensesWN\n",
    "Output: a mapping µ : SensesWiki → SensesWN\n",
    "1: for each w ∈ SensesWiki\n",
    "2: µ(w) := \u000f\n",
    "3: for each w ∈ SensesWiki\n",
    "4: if |SensesWiki(w)| = |SensesWN(w)| = 1 then\n",
    "5: µ(w) := w\n",
    "1\n",
    "n\n",
    "6: for each w ∈ SensesWiki\n",
    "7: if µ(w) = \u000f then\n",
    "8: for each d ∈ SensesWiki s.t. d redirects to w\n",
    "9: if µ(d) 6= \u000f and µ(d) is in a synset of w then\n",
    "10: µ(w) := sense of w in synset of µ(d); break\n",
    "11: for each w ∈ SensesWiki\n",
    "12: if µ(w) = \u000f then\n",
    "13: if no tie occurs then\n",
    "14: µ(w) := argmax\n",
    "s∈SensesWN(w)\n",
    "p(s|w)\n",
    "15: return µ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
