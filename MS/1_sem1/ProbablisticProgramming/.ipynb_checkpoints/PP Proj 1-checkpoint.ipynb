{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wikipedia'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-331-863a5885150f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wikipedia'"
     ]
    }
   ],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import copy \n",
    "\n",
    "import os\n",
    "import wikipedia\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [[\"aaa\", \"bbb\", \"aaa\"], \n",
    "        [\"bbb\", \"aaa\", \"bbb\"], \n",
    "        [\"aaa\", \"bbb\", \"bbb\", \"aaa\"], \n",
    "        [\"uuu\", \"vvv\"], \n",
    "        [\"uuu\", \"vvv\", \"vvv\"], \n",
    "        [\"uuu\", \"vvv\", \"vvv\", \"uuu\"]]\n",
    "\n",
    "new_doc = [\"aaa\", \"bbb\", \"aaa\", \"bbb\", \"uuu\"]\n",
    "\n",
    "doc_test = \"What is the meaning of all this?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP MAGIC**\n",
    "\n",
    "We are going to apply some NLP magic, such as stemming, lemming and removing stopwords. This will greatly help us in better identifying the underlying topic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_data(doc):\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    only_words = tokenizer.tokenizer(doc)\n",
    "    print(only_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-327-123d2585a506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclear_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-324-dd03a5439ff0>\u001b[0m in \u001b[0;36mclear_data\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclear_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\w+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0monly_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monly_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "clear_data(doc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    def __init__(self, docs, K, alpha_scalar, beta_scalar):\n",
    "        \n",
    "        # K is the number of topics\n",
    "        self.K = K\n",
    "        \n",
    "        self.initialize_vocabulary(docs)\n",
    "        \n",
    "        self.alpha = np.ones(self.K) * alpha_scalar\n",
    "        self.beta = np.ones(self.V) * beta_scalar\n",
    "        \n",
    "    def initialize_vocabulary(self, docs):\n",
    "        self.vocabulary = self.create_vocabulary(docs)\n",
    "        self.index_data = self.replace_words_with_index(docs, self.vocabulary)\n",
    "        \n",
    "        # M is the number of docs\n",
    "        self.M = len(docs)\n",
    "        \n",
    "        # number of unique words\n",
    "        self.V = len(self.vocabulary)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def initialize_distributions(self):\n",
    "        # Theta is the document distribution over the topic space\n",
    "        self.theta_i = np.empty(self.M, dtype=object)\n",
    "        self.theta = np.empty(self.M, dtype=object)\n",
    "        for i in range(self.M):\n",
    "            self.theta_i[i] = pm.Dirichlet(\"theta_i_%i\" % i, theta = self.alpha)\n",
    "            \n",
    "        self.theta_i = pm.Container(self.theta_i)\n",
    "        \n",
    "        for i in range(self.M):\n",
    "            self.theta[i] = pm.CompletedDirichlet(\"theta_%i\" % i, self.theta_i[i])\n",
    "        \n",
    "        self.theta = pm.Container(self.theta)\n",
    "        \n",
    "        # Phi is the word distribution for the topic space\n",
    "        self.phi_i = np.empty(self.K, dtype=object)\n",
    "        self.phi = np.empty(self.K, dtype=object)\n",
    "        \n",
    "        \n",
    "        for i in range(self.K):\n",
    "            self.phi_i[i] = pm.Dirichlet(\"phi_i_%i\" % i, theta = self.beta)\n",
    "            \n",
    "        self.phi_i = pm.Container(self.phi_i)\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            self.phi[i] = pm.CompletedDirichlet(\"phi_%i\" % i, self.phi_i[i])\n",
    "        \n",
    "        self.phi = pm.Container(self.phi)\n",
    "        \n",
    "        # Zeeta is the word-topic mapping        \n",
    "        self.zeeta = np.empty(self.M, dtype=object)\n",
    "        for i in range(self.M):\n",
    "            self.zeeta[i] = pm.Categorical(\"zeeta_%i\" % i, p = self.theta[i], size = len(self.index_data[i]))\n",
    "            \n",
    "        self.zeeta = pm.Container(self.zeeta)\n",
    "        \n",
    "        \n",
    "        # W is the \n",
    "        self.w = pm.Container(\n",
    "            [                             # document d, word i\n",
    "                pm.Categorical(\"w_%i_%i\" % (d, i), \n",
    "                              p = pm.Lambda(\"phi_z_%i%i\" % (d, i), \n",
    "                                           lambda z = self.zeeta[d][i], phi = self.phi : phi[z]\n",
    "                                           ),\n",
    "                               value = self.index_data[d][i],\n",
    "                               observed = True\n",
    "                              )\n",
    "                for d in range(self.M) for i in range(len((self.index_data[d])))\n",
    "                    \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.model = pm.Model([self.theta_i, self.theta, self.phi_i, self.phi, self.zeeta, self.w])\n",
    "        self.mcmc = pm.MCMC(self.model)\n",
    "        \n",
    "    \n",
    "    def get_params(self):\n",
    "        theta_mean = []\n",
    "        for i in range(self.M):\n",
    "            theta_mean.append(self.mcmc.trace(\"theta_%i\" % i)[:].mean(axis=0))\n",
    "        \n",
    "        phi_mean = []\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            phi_mean.append(self.mcmc.trace(\"phi_%i\" % i)[:].mean(axis=0))\n",
    "        \n",
    "        return theta_mean, phi_mean\n",
    "    \n",
    "    def print_zeeta(self, size):\n",
    "        for i in range(self.M):\n",
    "            print(\"Doc_%i\" % i)\n",
    "            print(self.mcmc.trace(\"zeeta_%i\" % i)[0:size])\n",
    "        \n",
    "    def create_vocabulary(self, data):\n",
    "        index = 0\n",
    "        vocabulary = {}\n",
    "        for document in data:\n",
    "            for word in document:\n",
    "                if (word not in vocabulary.keys()):\n",
    "                    vocabulary[word] = index\n",
    "                    index += 1\n",
    "        return vocabulary\n",
    "\n",
    "    \n",
    "    def replace_words_with_index(self, data, vocabulary):\n",
    "        # damn it, python!\n",
    "        index_data = copy.deepcopy(data)\n",
    "        for doc_index, document in enumerate(data):\n",
    "            for word_index, word in enumerate(document):\n",
    "                index_data[doc_index][word_index] = vocabulary[word]\n",
    "\n",
    "        return index_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**\n",
    "- Build the observed variable\n",
    "- Inferd the hidden topic structure\n",
    "- Trace also z (this is quite hard to print in a useful manner - what did you have in mind?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LDA(docs, 2, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA.initialize_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 10000 of 10000 complete in 13.1 sec"
     ]
    }
   ],
   "source": [
    "LDA.mcmc.sample(10000,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "(theta_mean, phi_mean) = LDA.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta mean:\n",
      "[[0.737883 0.262117]]\n",
      "[[0.84397158 0.15602842]]\n",
      "[[0.86136923 0.13863077]]\n",
      "[[0.19049577 0.80950423]]\n",
      "[[0.1585645 0.8414355]]\n",
      "[[0.10338018 0.89661982]]\n",
      "Phi mean:\n",
      "[[0.45263899 0.46864792 0.0172616  0.06145149]]\n",
      "[[0.10632954 0.05417827 0.39990943 0.43958276]]\n",
      "Zeeta\n",
      "Doc_0\n",
      "[[0 0 0]\n",
      " [0 0 0]]\n",
      "Doc_1\n",
      "[[0 0 0]\n",
      " [0 0 0]]\n",
      "Doc_2\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "Doc_3\n",
      "[[1 1]\n",
      " [1 1]]\n",
      "Doc_4\n",
      "[[1 1 1]\n",
      " [1 1 1]]\n",
      "Doc_5\n",
      "[[1 1 1 1]\n",
      " [1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Theta mean:\")\n",
    "for item in theta_mean:\n",
    "    print(item)\n",
    "    \n",
    "print(\"Phi mean:\")\n",
    "for item in phi_mean:\n",
    "    print(item)\n",
    "    \n",
    "print(\"Zeeta\")\n",
    "LDA.print_zeeta(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can the topic model be used to define a topic-based similarity measure between documents?**\n",
    "\n",
    "In order to mesaure the topic-based similarity between two documents, we have to measure the difference between their probability distributions. For this, we can use an f-divergence. Many common divergences, such as KL-Divergence, Hellinger distance and total variation distance are special cases of f-divergence, coinciding with a particular choice of f.\n",
    "\n",
    "More info can be found here: https://en.wikipedia.org/wiki/F-divergence\n",
    "\n",
    "I chose the Hellinger distance, as it seems to be the most popular one. https://en.wikipedia.org/wiki/Hellinger_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Hellinger_distance(LDA):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What about new documents? How can topics be assigned to it?**\n",
    "\n",
    "The easiest way to add new documents and assing topics to add the new document to the document\n",
    "list and run inference on them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 2000 of 2000 complete in 2.7 sec"
     ]
    }
   ],
   "source": [
    "docs.append(new_doc)\n",
    "LDA.initialize_vocabulary(docs)\n",
    "LDA.mcmc.sample(2000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
