{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab_5_Solved.ipynb","version":"0.3.2","provenance":[{"file_id":"1bTI3ZjGiy6rkopKVmtsZ3aaoqIFAymzn","timestamp":1544101655356}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"QbJto_4BzREm","colab_type":"text"},"cell_type":"markdown","source":["# Lab 5. Natural Language Processing. Unsupervised Learning"]},{"metadata":{"colab_type":"code","id":"ka3g6qXCcZ3_","colab":{}},"cell_type":"code","source":["# Some IPython magic\n","# Put these at the top of every notebook, here nbagg is used for interactive plots\n","%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"JBns_Q4qY2Fx"},"cell_type":"markdown","source":["## Natural Language Processing.\n","NLP refers to processing text data. This could refer to a wide range of tasks, from very simple ones, like searching for a pattern, to very complex ones, like text summarization, or automated translation.\n","\n","### Feature Extraction\n","In order to apply Machine Learning algorithms on text data, we need to figure out a way to represent the text as a set of numeric attributes.\n","\n","#### Bag of Words\n","The simplest way to represent a text document as a vector of numbers is to count the words, and output a frequency count. Let's say we have a list of all english words, like the following:"]},{"metadata":{"id":"aaExhqt5zRE0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ff9861fc-8235-4d0b-8755-e2919fab53ae","executionInfo":{"status":"ok","timestamp":1544113621627,"user_tz":-120,"elapsed":918,"user":{"displayName":"Andrei Iusan","photoUrl":"","userId":"08705953815839074682"}}},"cell_type":"code","source":["# creates a wordlist, with all words, from \"a\" to \"zygote\"\n","import urllib.request as request\n","words = request.urlopen(\"https://svnweb.freebsd.org/csrg/share/dict/words?view=co\")\n","wordlist = []\n","for w in words:\n","    wordlist.append(str(w.decode().strip()))\n","print(', '.join(wordlist[:4]) + \" ... \" + ', '.join(wordlist[-4:]))"],"execution_count":38,"outputs":[{"output_type":"stream","text":["a, AAA, AAAS, aardvark ... zucchini, Zulu, Zurich, zygote\n"],"name":"stdout"}]},{"metadata":{"id":"jgVJ47zczRE8","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"Now we can convert any text to a vector of size \" \\\n","      + str(len(wordlist)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"z4WzacIpzRFC","colab_type":"text"},"cell_type":"markdown","source":["For example, the text \"In this lab we study Natural Language Processing and Unsupervised Learning\" can be represented as a vector with almost all values equal to 0, and values of 1 in the position of the words \"in\", \"this\", etc.\n","\n","This ___feature vector___ can be extracted directly from the dataset. If we have a large collection of text, we can assume that other documents will use the same vocabulary. So if we build a model for news articles, most likely those articles will not use every single word in the english language. So during the training phase of our machine learning modeling, we can use the train set to create our feature vector. We select only the words that appear in the train set. If new words appear during the test phase, we will discard them. This is a good thing because during training, we did not learn anything about those words. We cannot use unseen words to perform classification.\n","\n","Let's start working with a dataset."]},{"metadata":{"colab_type":"code","id":"-BieAe7lWcWg","colab":{}},"cell_type":"code","source":["from sklearn.datasets import fetch_20newsgroups\n","\n","# We select only 3 categories for now, feel free to change the categories\n","categories = [\n","    'rec.sport.baseball',\n","    'comp.graphics',\n","    'sci.space',\n","]\n","dataset = fetch_20newsgroups(subset='all', categories=categories, \n","                              shuffle=True, random_state=42)\n","\n","#dataset = fetch_20newsgroups(subset='all', shuffle=True, random_state=42) #if you want all caterogies\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7pwUTmInbViU","colab_type":"code","colab":{}},"cell_type":"code","source":["# here are the attributes of the object retrieved by fetch_20newsgroups\n","dir(dataset)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_VKru4wJbHGF","colab_type":"code","colab":{}},"cell_type":"code","source":["len(dataset.data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ThoBpdCY6B4P","colab_type":"code","colab":{}},"cell_type":"code","source":["X = dataset.data\n","y = dataset.target"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j_Q_EQJsoDPd","colab_type":"code","colab":{}},"cell_type":"code","source":["print(dataset.data[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9lUSPGecpec7","colab_type":"code","colab":{}},"cell_type":"code","source":["dataset.target_names"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4aBr7a7M7WnM","colab_type":"text"},"cell_type":"markdown","source":["One way to turn a text document into a feature vector is to use a frequency count of\n","each word in the document.  We build a large dictionary of words, and for each document\n","we return a vector with as many features as there are words, and for each word, we return\n","the  number  of  times  that  word  appears  in  the  document  (this  is  technically  called\n","**term frequency** , or tf for short).  Sklearn has a **[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)**  that does just that."]},{"metadata":{"id":"xzguRZFUUv2h","colab_type":"code","outputId":"c5f93447-346f-49af-d883-753f4bddf37f","executionInfo":{"status":"ok","timestamp":1544113769312,"user_tz":-120,"elapsed":1516,"user":{"displayName":"Andrei Iusan","photoUrl":"","userId":"08705953815839074682"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["#TO DO: Transfrom the dataset into numerical feature vectors\n","from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(dataset.data)\n","vocabulary = vectorizer.get_feature_names()\n","print(vocabulary[:10])\n","print(vocabulary[-10:])"],"execution_count":41,"outputs":[{"output_type":"stream","text":["['00', '000', '0000', '00000', '000000', '000005102000', '000021', '000050', '000062david42', '0000ahc']\n","['zwarte', 'zwork', 'zyda', 'zyeh', 'zyxel', 'zz', 'zzzzzz', 'zzzzzzt', 'ªl', 'ñaustin']\n"],"name":"stdout"}]},{"metadata":{"id":"vYGuDVmznrYg","colab_type":"text"},"cell_type":"markdown","source":["We can see there are many examples with digits or special characters ('ñaustin'). We could try to improve the vectorizer by removing some characters, but we will do this with TfIdf below."]},{"metadata":{"id":"d-NxB5KHT579","colab_type":"text"},"cell_type":"markdown","source":["One problem with this representation is the high frequency of common words like ”the” or ”to” or ”and”.  Those words appear in almost all documents, so they don’t offer much information.\n","A better way to extract features from text is to use both the **term frequency** metric and the **inverse document frequency** metric . Sklearn has a **[TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)** that does just that."]},{"metadata":{"id":"OP3srS8-7EYf","colab_type":"code","outputId":"abb56b68-2c17-4867-dd06-78b5817fcd9d","executionInfo":{"status":"ok","timestamp":1544104122267,"user_tz":-120,"elapsed":1478,"user":{"displayName":"Andrei Iusan","photoUrl":"","userId":"08705953815839074682"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["#TO DO: Transfrom the dataset into tf-idf feature vectors\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(dataset.data)\n","vocabulary = vectorizer.get_feature_names()\n","print(vocabulary[:10])\n","print(vocabulary[-10:])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['00', '000', '0000', '00000', '000000', '000005102000', '000021', '000050', '000062david42', '0000ahc']\n","['zwarte', 'zwork', 'zyda', 'zyeh', 'zyxel', 'zz', 'zzzzzz', 'zzzzzzt', 'ªl', 'ñaustin']\n"],"name":"stdout"}]},{"metadata":{"id":"2z7_MxEXpp4G","colab_type":"text"},"cell_type":"markdown","source":["Let's improve the embedding by tweaking the TfidfVectorizer."]},{"metadata":{"id":"F_Eofkxvpx7d","colab_type":"code","outputId":"70b24c9e-0ade-400d-d9fc-25ad34648d40","executionInfo":{"status":"ok","timestamp":1544113841855,"user_tz":-120,"elapsed":1435,"user":{"displayName":"Andrei Iusan","photoUrl":"","userId":"08705953815839074682"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer(strip_accents='ascii', stop_words='english', token_pattern=r'(?u)\\b[A-Za-z]+\\b')\n","X = vectorizer.fit_transform(dataset.data)\n","vocabulary = vectorizer.get_feature_names()\n","print(vocabulary[:10])\n","print(vocabulary[-10:])"],"execution_count":42,"outputs":[{"output_type":"stream","text":["['aa', 'aaa', 'aaaa', 'aaaaarrrrgh', 'aaai', 'aaauuugggghhhhh', 'aachen', 'aacs', 'aad', 'aalborg']\n","['zwakke', 'zware', 'zwarte', 'zwork', 'zyda', 'zyeh', 'zyxel', 'zz', 'zzzzzz', 'zzzzzzt']\n"],"name":"stdout"}]},{"metadata":{"id":"ygtd6vFvxQbh","colab_type":"text"},"cell_type":"markdown","source":["And now we have words with only alphabetical lowercase characters."]},{"metadata":{"id":"6ukWHfO-7me2","colab_type":"text"},"cell_type":"markdown","source":["Now you will need to use the vectorized dataset to perfom clustering.\n","\n","You will need to  use the following algorithms : [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html), [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.k_means.html#sklearn.cluster.k_means), [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering).\n","\n","For each algorithm try to find the parameters that produce clusters as similar as possible to the real distribution of the data.\n","\n","Use different metrics to evaluate the algorithms : [Rand Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html),  [Silhouette Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html), [Homogeneity Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html), [Completness Score.](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score)\n","\n","\n","\n","\n"]},{"metadata":{"id":"Fs6scxqoRJpc","colab_type":"code","colab":{}},"cell_type":"code","source":["# TODO : Use the following algorithms to perform clustering on the dataset  \n","from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n","from sklearn.metrics import silhouette_score, adjusted_rand_score, homogeneity_score, completeness_score\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"puzQdk5K3d20","colab_type":"code","outputId":"cb95f7e5-e042-4b82-d67b-b9f67771bbcf","executionInfo":{"status":"ok","timestamp":1544113854591,"user_tz":-120,"elapsed":1301,"user":{"displayName":"Andrei Iusan","photoUrl":"","userId":"08705953815839074682"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# DBSCAN out of the box:\n","dbscan = DBSCAN()\n","dbscan.fit(X)\n","unique, counts = np.unique(dbscan.labels_, return_counts=True)\n","print(\"clusters:\", dict(zip(unique, counts)))\n"],"execution_count":44,"outputs":[{"output_type":"stream","text":["clusters: {-1: 2941, 0: 6, 1: 7}\n"],"name":"stdout"}]},{"metadata":{"id":"2o11wXoU4Ib0","colab_type":"text"},"cell_type":"markdown","source":["Most points are marked as outliers. Clearly we should increase the radious. So let's try different values and store the clusterings."]},{"metadata":{"id":"mXPSGna73yIp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":263},"outputId":"669217ea-39b9-4222-f337-f1793ab9aa11","executionInfo":{"status":"ok","timestamp":1544114272421,"user_tz":-120,"elapsed":10532,"user":{"displayName":"Andrei Iusan","photoUrl":"","userId":"08705953815839074682"}}},"cell_type":"code","source":["# we make a list of clusterings and store each clustering as a dictionary \n","# with relevant information.\n","\n","clusterings = []\n","\n","epsilon = np.arange(0.7, 2, 0.1)\n","for e in epsilon:\n","  cluster = {}\n","  dbscan = DBSCAN(eps=e)\n","  dbscan.fit(X)\n","  cluster[\"algorithm\"] = \"DBSCAN\"\n","  cluster[\"labels\"] = dbscan.labels_\n","  cluster[\"parameters\"] = {'eps': e}\n","  clusterings.append(cluster)\n","  \n","  unique, counts = np.unique(dbscan.labels_, return_counts=True)\n","  print(\"eps:\",e,\"clusters:\", dict(zip(unique, counts)))"],"execution_count":47,"outputs":[{"output_type":"stream","text":["eps: 0.7 clusters: {-1: 2924, 0: 5, 1: 6, 2: 6, 3: 7, 4: 6}\n","eps: 0.7999999999999999 clusters: {-1: 2884, 0: 6, 1: 5, 2: 6, 3: 6, 4: 6, 5: 6, 6: 7, 7: 7, 8: 6, 9: 5, 10: 5, 11: 5}\n","eps: 0.8999999999999999 clusters: {-1: 2772, 0: 8, 1: 9, 2: 7, 3: 8, 4: 6, 5: 7, 6: 7, 7: 11, 8: 8, 9: 6, 10: 6, 11: 6, 12: 7, 13: 8, 14: 6, 15: 5, 16: 5, 17: 9, 18: 5, 19: 5, 20: 5, 21: 6, 22: 5, 23: 6, 24: 6, 25: 5, 26: 5, 27: 5}\n","eps: 0.9999999999999999 clusters: {-1: 2502, 0: 5, 1: 8, 2: 9, 3: 12, 4: 8, 5: 7, 6: 7, 7: 17, 8: 7, 9: 21, 10: 8, 11: 7, 12: 8, 13: 7, 14: 21, 15: 7, 16: 8, 17: 8, 18: 14, 19: 7, 20: 6, 21: 7, 22: 8, 23: 6, 24: 7, 25: 9, 26: 6, 27: 5, 28: 6, 29: 5, 30: 5, 31: 5, 32: 7, 33: 21, 34: 6, 35: 10, 36: 6, 37: 5, 38: 9, 39: 7, 40: 5, 41: 5, 42: 7, 43: 7, 44: 11, 45: 7, 46: 10, 47: 5, 48: 7, 49: 6, 50: 5, 51: 5, 52: 5, 53: 5, 54: 5, 55: 5, 56: 5, 57: 5}\n","eps: 1.0999999999999999 clusters: {-1: 1878, 0: 333, 1: 17, 2: 17, 3: 13, 4: 8, 5: 8, 6: 17, 7: 23, 8: 40, 9: 63, 10: 5, 11: 14, 12: 102, 13: 31, 14: 10, 15: 11, 16: 10, 17: 9, 18: 11, 19: 8, 20: 8, 21: 7, 22: 9, 23: 13, 24: 6, 25: 6, 26: 13, 27: 9, 28: 8, 29: 7, 30: 6, 31: 7, 32: 4, 33: 7, 34: 7, 35: 5, 36: 6, 37: 7, 38: 13, 39: 5, 40: 4, 41: 7, 42: 6, 43: 8, 44: 5, 45: 6, 46: 8, 47: 4, 48: 9, 49: 4, 50: 5, 51: 5, 52: 6, 53: 5, 54: 5, 55: 7, 56: 3, 57: 4, 58: 6, 59: 6, 60: 8, 61: 5, 62: 3, 63: 5, 64: 5, 65: 6, 66: 6, 67: 4, 68: 4, 69: 4, 70: 5, 71: 5}\n","eps: 1.1999999999999997 clusters: {-1: 1088, 0: 1587, 1: 13, 2: 5, 3: 8, 4: 4, 5: 15, 6: 12, 7: 6, 8: 6, 9: 5, 10: 13, 11: 17, 12: 6, 13: 9, 14: 4, 15: 14, 16: 8, 17: 12, 18: 8, 19: 5, 20: 6, 21: 8, 22: 7, 23: 6, 24: 5, 25: 6, 26: 4, 27: 6, 28: 11, 29: 8, 30: 7, 31: 4, 32: 5, 33: 5, 34: 7, 35: 5, 36: 4, 37: 5}\n","eps: 1.2999999999999998 clusters: {-1: 236, 0: 2702, 1: 5, 2: 7, 3: 4}\n","eps: 1.4 clusters: {0: 2954}\n","eps: 1.4999999999999998 clusters: {0: 2954}\n","eps: 1.5999999999999996 clusters: {0: 2954}\n","eps: 1.6999999999999997 clusters: {0: 2954}\n","eps: 1.7999999999999996 clusters: {0: 2954}\n","eps: 1.8999999999999997 clusters: {0: 2954}\n"],"name":"stdout"}]},{"metadata":{"id":"taYS_xxT9DTv","colab_type":"text"},"cell_type":"markdown","source":["We still don't obtain good results. We obtain either a lot of outliers, either many small clusters, or one big cluster."]},{"metadata":{"id":"RUqyMicB9Kep","colab_type":"code","outputId":"caf49848-b1e6-425a-932d-0c0226d30cf0","executionInfo":{"status":"ok","timestamp":1544107294765,"user_tz":-120,"elapsed":56636,"user":{"displayName":"Andrei Iusan","photoUrl":"","userId":"08705953815839074682"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["kmeans = KMeans(n_clusters=3)\n","kmeans.fit(X)\n","unique, counts = np.unique(kmeans.labels_, return_counts=True)\n","print(\"clusters:\", dict(zip(unique, counts)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["clusters: {0: 539, 1: 719, 2: 1696}\n"],"name":"stdout"}]},{"metadata":{"id":"g83dHQqa-Gl4","colab_type":"text"},"cell_type":"markdown","source":["KMeans always produces the number of clusters we set it to produce. Let's vary the number of clusters and print the Silhouette Score. We can see from the documentation that this is the only metric of the mentioned ones that takes only the clustered data as input, and attempts to measure the fitness of the clustering. The other metrics compare a clustering with a ground truth. We will use them to test whether the clustering coincides with the topics of the documents."]},{"metadata":{"id":"zfcrnVeN-m0k","colab_type":"code","outputId":"9604bd76-f74f-49bc-983c-cd3f3e4a92dc","executionInfo":{"status":"ok","timestamp":1544108284093,"user_tz":-120,"elapsed":38917,"user":{"displayName":"Andrei Iusan","photoUrl":"","userId":"08705953815839074682"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["scores = []\n","for n in range(2,10):\n","  kmeans = KMeans(n_clusters=n)\n","  kmeans.fit(X)\n","  s=silhouette_score(X,kmeans.labels_)\n","  agg_scores.append((n,s))\n","  print(n,':',s)\n","  \n","  cluster[\"algorithm\"] = \"KMeans\"\n","  cluster[\"labels\"] = kmeans.labels_\n","  cluster[\"parameters\"] = {'n_clusters': n}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2 : 0.004192722109319261\n"],"name":"stdout"}]},{"metadata":{"id":"g4dvEO5FCUEg","colab_type":"text"},"cell_type":"markdown","source":["All kmeans clusterings produce near zero silhouette scores. This means overlapping clusters."]},{"metadata":{"id":"zDiVjEf9CoLD","colab_type":"code","outputId":"8af53e24-1acd-4f49-d646-7d9fc24527c9","executionInfo":{"status":"ok","timestamp":1544117744266,"user_tz":-120,"elapsed":7,"user":{"displayName":"Andrei Iusan","photoUrl":"","userId":"08705953815839074682"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Agglomerative clustering takes too much to run mutiple times.\n","agg = AgglomerativeClustering(n_clusters=3)\n","agg.fit(X.toarray())\n","s=silhouette_score(X,agg.labels_)\n","print(s)\n","\n","cluster[\"algorithm\"] = \"Agglomerative\"\n","cluster[\"labels\"] = agg.labels_\n","cluster[\"parameters\"] = {'n_clusters': 3}"],"execution_count":55,"outputs":[{"output_type":"stream","text":["0.005265867921220907\n"],"name":"stdout"}]},{"metadata":{"id":"IH58WoJDaP1H","colab_type":"text"},"cell_type":"markdown","source":["Let's compare K-Means and Agglomerative Clustering with 3 clusters with the original classes."]},{"metadata":{"id":"y9eGE-i0aeQ6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"outputId":"5f0ebdd9-5495-41b9-e990-ed6fb9b7e35f","executionInfo":{"status":"ok","timestamp":1544117852117,"user_tz":-120,"elapsed":652,"user":{"displayName":"Andrei Iusan","photoUrl":"","userId":"08705953815839074682"}}},"cell_type":"code","source":["relevant_clusterings = [c for c in clusterings \n","                        if c['algorithm'] == 'KMeans' and \n","                        c['parameters']['n_clusters'] == 3 or\n","                        c['algorithm'] == 'Agglomerative' and\n","                        c['parameters']['n_clusters'] == 3]\n","\n","# for metric in [adjusted_rand_score, homogeneity_score, completeness_score]:\n","for c in relevant_clusterings:\n","  print(c['algorithm'],':')\n","  print('rand score :', adjusted_rand_score(y, c['labels']))\n","  print('homogenity :', homogeneity_score(y, c['labels']))\n","  print('completeness :', adjusted_rand_score(y, c['labels']))"],"execution_count":56,"outputs":[{"output_type":"stream","text":["Agglomerative :\n","rand score : 0.5659118196442221\n","homogenity : 0.5337938117724491\n","completeness : 0.5659118196442221\n"],"name":"stdout"}]},{"metadata":{"id":"zmI2fRHhbBMq","colab_type":"text"},"cell_type":"markdown","source":["As you can see, high dimensional sparse vectors do not produce the best clusters.\n","Now, try to improve your results by using [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to reduce the dimensions of your feature vectors before applying the clustering algorithms. \n","\n","\n","\n"]},{"metadata":{"id":"fDLSqP34cB6d","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.decomposition import PCA"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PteL66ZWmBsc","colab_type":"text"},"cell_type":"markdown","source":["Use t-SNE to produce a low-dimensional embedding of the dataset (and plot it)."]},{"metadata":{"id":"vSXbJgJml8Zq","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.manifold import TSNE"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kRsrJol6cmUO","colab_type":"text"},"cell_type":"markdown","source":["As an extra exercise, try to implement kernel KMeans. Look at the KMeans course. Slide 70"]},{"metadata":{"colab_type":"code","id":"4mT5S8ZYcRxo","colab":{}},"cell_type":"code","source":["from sklearn.datasets import make_circles\n","\n","X, _ = make_circles(n_samples=100, noise=0.1, factor=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4liSXptEdW0g","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.metrics.pairwise import rbf_kernel\n","\n","\n","k = 2\n","# TODO : assign points to random clusters\n","y = \n","dist = np.zeros((X.shape[0], k))\n","\n","        \n","# TODO\n","max_iter = 10      \n","for _ in range(max_iter):      \n","  for j in range(k):\n","    # TODO : get the points that are in cluster j\n","    X_j = \n","    \n","    # TODO : compute the first term\n","          \n","    first_term = \n","    \n","    # TODO : compute the second term\n","    \n","    second_term = \n","      \n","    dist[:, j] = first_term + second_term\n","        \n","  # TODO : change the clusters\n","  y = np.argmin(....)\n","\n","   "],"execution_count":0,"outputs":[]},{"metadata":{"id":"s-MP7BYOCgjx","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.scatter(X[:,0], X[:,1], c=y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_S4QR2WFImsS","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"fgu_sSP1IoC0","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}